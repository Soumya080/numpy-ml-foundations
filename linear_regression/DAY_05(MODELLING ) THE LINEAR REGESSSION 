{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNmUCO5AKI9dDCUxrDJ/HN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumya080/numpy-ml-foundations/blob/main/linear_regression/DAY_05(MODELLING%20)%20THE%20LINEAR%20REGESSSION%20\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Global parameters\n",
        "num_iterations = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 1. Define x_data and y_data once as fixed input for training\n",
        "x_data = np.random.randn(100, 1)\n",
        "y_data = 2.5 * x_data + np.random.randn(100, 1)\n",
        "\n",
        "\n",
        "# Spliting  the data into training and testing sets (e.g., 80% train, 20% test)\n",
        "split_idx = int(0.8 * len(x_data))\n",
        "\n",
        "x_train = x_data[:split_idx]\n",
        "y_train = y_data[:split_idx]\n",
        "\n",
        "x_test = x_data[split_idx:]\n",
        "y_test = y_data[split_idx:]\n",
        "\n",
        "# Group x and y for training and testing\n",
        "train_data = [x_train, y_train]\n",
        "test_data = [x_test, y_test]\n",
        "\n",
        "print(\"Corrected Test Data structure:\")\n",
        "print(f\"  Shape of x_train: {x_train.shape}\")\n",
        "print(f\"  Shape of y_train: {y_train.shape}\")\n",
        "print(f\"  Shape of x_test: {x_test.shape}\")\n",
        "print(f\"  Shape of y_test: {y_test.shape}\")\n",
        "# print(test_data)\n",
        "\n",
        "\n",
        "# Original content of this cell (function definitions):\n",
        "# Function to initialize weights and bias\n",
        "def initialize_parameters():\n",
        "  w = np.random.randn(1, 1)\n",
        "  b = np.random.randn(1, 1)\n",
        "  return w, b\n",
        "\n",
        "# Function to compute loss\n",
        "def compute_loss(x, y, w, b):\n",
        "  m = x.shape[0]\n",
        "  y_pred = x @ w + b\n",
        "  loss = np.mean((y_pred - y)**2)\n",
        "  return loss\n",
        "\n",
        "# Function to compute gradient\n",
        "def compute_gradient(x, y, w, b):\n",
        "  m = x.shape[0]\n",
        "  y_pred = x @ w + b\n",
        "\n",
        "  grad_w = np.mean(2 * x * (y_pred - y))\n",
        "  grad_b = np.mean(2 * (y_pred - y))\n",
        "\n",
        "  return grad_w, grad_b\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WFf50NfZ03e",
        "outputId": "c0b56547-7b5c-4a59-ae47-1e81c09b7fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Test Data structure:\n",
            "  Shape of x_train: (80, 1)\n",
            "  Shape of y_train: (80, 1)\n",
            "  Shape of x_test: (20, 1)\n",
            "  Shape of y_test: (20, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to update parameters\n",
        "def update_parameters(w, b, grad_w, grad_b, learning_rate):\n",
        "  # learning_rate is passed as argument, no need to re-define here\n",
        "  w = w - learning_rate * grad_w\n",
        "  b = b - learning_rate * grad_b\n",
        "  return w, b\n",
        "\n",
        "# Training function\n",
        "def train(x, y, w_init, b_init, learning_rate, num_iterations):\n",
        "  w, b = w_init, b_init # Start with initial weights and bias\n",
        "  losses = [] # To store loss at each iteration\n",
        "  for i in range(num_iterations):\n",
        "    loss_val = compute_loss(x, y, w, b)\n",
        "    grad_w, grad_b = compute_gradient(x, y, w, b)\n",
        "    w, b = update_parameters(w, b, grad_w, grad_b, learning_rate)\n",
        "    losses.append(loss_val)\n",
        "  return w, b, losses # Return updated parameters and all recorded losses\n",
        "\n",
        "# Main execution block to run training and output results\n",
        "# Initialize weights and bias once before training\n",
        "initial_w, initial_b = initialize_parameters()\n",
        "\n",
        "# Run training\n",
        "final_w, final_b, all_losses = train(x_train, y_train, initial_w, initial_b, learning_rate, num_iterations)\n",
        "# Output results\n",
        "print(f\"Final Weights (w): {final_w}\")\n",
        "print(f\"Final Bias (b): {final_b}\")\n",
        "print(f\"Mean of all recorded losses during training: {np.mean(all_losses)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xvj7tK8Yj_g",
        "outputId": "6ef6600d-cb51-4f58-d60c-9e3118201639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights (w): [[2.39213585]]\n",
            "Final Bias (b): [[-0.02430264]]\n",
            "Mean of all recorded losses during training: 1.1658584251751887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TESTING\n",
        "final_w, final_b, all_losses = train(x_test, y_test, initial_w, initial_b, learning_rate, num_iterations)\n",
        "# Output results\n",
        "print(f\"Final Weights (w): {final_w}\")\n",
        "print(f\"Final Bias (b): {final_b}\")\n",
        "print(f\"Mean of all recorded losses during testing: {np.mean(all_losses)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAC_fCEAjpn5",
        "outputId": "cdb55555-56fd-47cf-ae92-d92c49d1d18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights (w): [[2.39655261]]\n",
            "Final Bias (b): [[0.29637122]]\n",
            "Mean of all recorded losses during testing: 1.4645389548598422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions for train and test\n",
        "y_train_pred = x_train @ final_w + final_b\n",
        "y_test_pred = x_test @ final_w + final_b\n",
        "\n",
        "# MAE calculation\n",
        "train_mae = np.mean(np.abs(y_train - y_train_pred))\n",
        "test_mae = np.mean(np.abs(y_test - y_test_pred))\n",
        "\n",
        "print(\"Train MAE:\", train_mae)\n",
        "print(\"Test MAE:\", test_mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crseiqhlkJzh",
        "outputId": "ecdccc53-2981-46da-8983-058ddcd93624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MAE: 0.7943075970140988\n",
            "Test MAE: 0.89697213248679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dE9oKYHClOi_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}